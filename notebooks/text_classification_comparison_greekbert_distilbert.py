# -*- coding: utf-8 -*-
"""Text Classification Comparison GreekBERT - DistilBERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ybh-jRqdJDltvKKNrYaQL4DnWA9LyTzg
"""

!nvidia-smi

# Step 1: Mount Google Drive
# from google.colab import drive
# import os

# drive.mount('/content/drive')

# Replace with your JSON file path in Google Drive
json_file_path = "/content/drive/MyDrive/datasets/data.json"

# Step 2: Install Required Libraries
!pip install transformers datasets scikit-learn evaluate

from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
from evaluate import load
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd
import json
import numpy as np
import matplotlib.pyplot as plt
import itertools

# Step 3: Load and Preprocess Data
def filter_data(json_file_path):
    with open(json_file_path, "r") as f:
        data = json.load(f)
    return (doc for doc in data if "processed_text" in doc and "assigned_category" in doc)

filtered_data = list(filter_data(json_file_path))
texts = [doc["processed_text"] for doc in filtered_data]
labels = [doc["assigned_category"] for doc in filtered_data]

# Preprocess labels
unique_labels = sorted(set(labels))
label_to_id = {label: i for i, label in enumerate(unique_labels)}
id_to_label = {i: label for label, i in label_to_id.items()}  # Reverse mapping
labels = [label_to_id[label] for label in labels]

# Split into train, validation, and test sets
from sklearn.model_selection import train_test_split
train_texts, temp_texts, train_labels, temp_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)
val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)

# Step 4: Tokenization
model_names = ["nlpaueb/bert-base-greek-uncased-v1", "distilbert-base-multilingual-cased"]
tokenizers = [AutoTokenizer.from_pretrained(name) for name in model_names]

# Tokenize datasets
def tokenize_data(tokenizer, texts, labels):
    tokenized_data = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors='pt')
    tokenized_data["labels"] = labels
    return Dataset.from_dict(tokenized_data)

train_data = [tokenize_data(tokenizer, train_texts, train_labels) for tokenizer in tokenizers]
val_data = [tokenize_data(tokenizer, val_texts, val_labels) for tokenizer in tokenizers]
test_data = [tokenize_data(tokenizer, test_texts, test_labels) for tokenizer in tokenizers]

# Step 5: Load Models
models = [AutoModelForSequenceClassification.from_pretrained(name, num_labels=len(label_to_id)) for name in model_names]

# Step 6: Training and Evaluation
def train_and_evaluate(model, tokenizer, train_dataset, val_dataset, test_dataset, id_to_label):
    metric = load('accuracy')

    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        predictions = np.argmax(logits, axis=1)
        accuracy = metric.compute(predictions=predictions, references=labels)
        return {"accuracy": accuracy["accuracy"]}

    training_args = TrainingArguments(
        output_dir="./results",
        evaluation_strategy="epoch",
        save_strategy="epoch",
        learning_rate=2e-5,
        per_device_train_batch_size=16,
        num_train_epochs=5,
        weight_decay=0.01,
        logging_dir="./logs",
        logging_steps=10,
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
        report_to="none",
        fp16=True,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    trainer.train()

    # Evaluate on test set
    predictions = trainer.predict(test_dataset)
    preds = np.argmax(predictions.predictions, axis=1)
    metrics = classification_report(
        test_dataset["labels"],
        preds,
        target_names=[id_to_label[i] for i in range(len(id_to_label))],
        output_dict=True
    )

    return metrics, confusion_matrix(test_dataset["labels"], preds)

# Step 7: Run for Both Models
results = {}
for i, model in enumerate(models):
    metrics, cm = train_and_evaluate(model, tokenizers[i], train_data[i], val_data[i], test_data[i], id_to_label)
    results[model_names[i]] = {"metrics": metrics, "confusion_matrix": cm}

# Save results to a file
results_file = '/content/drive/MyDrive/datasets/classification_results.json'
# Convert confusion matrices to lists for JSON serialization
for model_name in results:
    results[model_name]["confusion_matrix"] = results[model_name]["confusion_matrix"].tolist()

with open(results_file, 'w') as f:
    json.dump(results, f)

# Print metrics and confusion matrices
for model_name, result in results.items():
    print(f"\nResults for {model_name}")
    print("Metrics:")
    print(result["metrics"])
    print("Confusion Matrix:")
    print(result["confusion_matrix"])

# Evaluate on validation set
results = trainer.evaluate()

# Print accuracy and loss
print(f"Validation Loss: {results['eval_loss']}")
print(f"Validation Accuracy: {results['eval_accuracy']}")

# Step 8: Visualize Confusion Matrix
def plot_confusion_matrix(cm, labels):
    cm = np.array(cm)  # Convert cm to NumPy array
    plt.figure(figsize=(10, 8))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('Confusion Matrix', fontsize=16)
    plt.colorbar()

    tick_marks = np.arange(len(labels))
    plt.xticks(tick_marks, labels, rotation=45, ha='right', fontsize=12)  # Adjust rotation and alignment
    plt.yticks(tick_marks, labels, fontsize=12)

    fmt = 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black",
                 fontsize=10)

    plt.tight_layout()  # Ensures labels and title fit properly
    plt.ylabel('True Label', fontsize=14)
    plt.xlabel('Predicted Label', fontsize=14)
    plt.show()


for model_name, result in results.items():
    print(f"Confusion Matrix for {model_name}")
    plot_confusion_matrix(result["confusion_matrix"], [id_to_label[i] for i in range(len(id_to_label))])